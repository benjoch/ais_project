{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define paths\n",
    "transcripts_dir = \"Transcripts\" \n",
    "\n",
    "# Step 1: Load and preprocess transcripts\n",
    "def load_transcripts(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "def split_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "print(\"Loading and preprocessing transcripts...\")\n",
    "transcripts = load_transcripts(transcripts_dir)\n",
    "chunks = [chunk for text in transcripts for chunk in split_into_chunks(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create embeddings and store in FAISS\n",
    "print(\"Creating embeddings and storing in FAISS...\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define retrieval function\n",
    "def retrieve_relevant_chunks(query, k=5):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    results = [{\"text\": chunks[i], \"distance\": distances[0][j]} for j, i in enumerate(indices[0])]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define LLM interface\n",
    "class LLMInterface:\n",
    "    def __init__(self, model_name=\"google/flan-t5-small\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.generator = pipeline(\"text2text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def generate_response(self, query, context_chunks):\n",
    "        context = \"\\n\\n\".join([chunk[\"text\"] for chunk in context_chunks])\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert financial analyst. Based on the following context from earnings call transcripts, answer the question:\n",
    "        Context:\n",
    "        {context}\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "        response = self.generator(prompt, max_length=300, truncation=True)\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLMInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define analytics\n",
    "def analyze_response(query, context_chunks, response):\n",
    "    analytics = {\n",
    "        \"query\": query,\n",
    "        \"context_size\": len(context_chunks),\n",
    "        \"retrieved_characters\": sum(len(chunk[\"text\"]) for chunk in context_chunks),\n",
    "        \"response_length\": len(response),\n",
    "        \"response\": response,\n",
    "    }\n",
    "    return analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run retrieval, generation, and analytics\n",
    "query = \"What are the key risks mentioned in the earnings calls?\"\n",
    "print(\"Retrieving relevant chunks...\")\n",
    "start_retrieval = time.time()\n",
    "context_chunks = retrieve_relevant_chunks(query)\n",
    "end_retrieval = time.time()\n",
    "\n",
    "print(\"Generating response...\")\n",
    "start_generation = time.time()\n",
    "response = llm.generate_response(query, context_chunks)\n",
    "end_generation = time.time()\n",
    "\n",
    "# Print response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)\n",
    "\n",
    "# Print analytics\n",
    "analytics = analyze_response(query, context_chunks, response)\n",
    "analytics[\"retrieval_time\"] = end_retrieval - start_retrieval\n",
    "analytics[\"generation_time\"] = end_generation - start_generation\n",
    "print(\"\\nAnalytics:\")\n",
    "for key, value in analytics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define sub-criteria and queries\n",
    "sub_criteria = {\n",
    "    \"Growth Potential\": \"What are the company's growth opportunities mentioned in the earnings calls?\",\n",
    "    \"Management Quality\": \"What insights about management effectiveness are mentioned in the earnings calls?\",\n",
    "    \"Earnings Quality\": \"What are the key factors influencing the company's earnings quality?\",\n",
    "    \"Business Risks\": \"What risks were highlighted in the earnings calls?\",\n",
    "}\n",
    "\n",
    "# Function to generate a score for a sub-criterion\n",
    "def generate_score_and_reasoning(query, context_chunks):\n",
    "    response = llm.generate_response(query, context_chunks)\n",
    "    \n",
    "    # Simulate score extraction (you can parse this more intelligently if needed)\n",
    "    # For simplicity, assume the model outputs a score as part of the reasoning.\n",
    "    score = float(response.split(\"Score:\")[1].strip().split()[0]) if \"Score:\" in response else None\n",
    "    return {\"score\": score, \"reasoning\": response}\n",
    "\n",
    "# Function to calculate AIS for a single transcript\n",
    "def calculate_ais(transcript_id, transcript_text):\n",
    "    results = {\"Transcript ID\": transcript_id}\n",
    "    transcript_chunks = split_into_chunks(transcript_text)\n",
    "\n",
    "    # Encode and store chunks in FAISS for the specific transcript\n",
    "    transcript_embeddings = embedding_model.encode(transcript_chunks, convert_to_numpy=True)\n",
    "    faiss_index.add(transcript_embeddings)\n",
    "\n",
    "    for criterion, query in sub_criteria.items():\n",
    "        # Retrieve relevant chunks\n",
    "        context_chunks = retrieve_relevant_chunks(query)\n",
    "        \n",
    "        # Generate score and reasoning\n",
    "        result = generate_score_and_reasoning(query, context_chunks)\n",
    "        results[f\"{criterion} Score\"] = result[\"score\"]\n",
    "        results[f\"{criterion} Reasoning\"] = result[\"reasoning\"]\n",
    "    \n",
    "    # Calculate final AIS (average of sub-criteria scores)\n",
    "    results[\"Final AIS\"] = sum(results[f\"{criterion} Score\"] for criterion in sub_criteria if results[f\"{criterion} Score\"] is not None) / len(sub_criteria)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
